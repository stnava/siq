window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "siq", "modulename": "siq", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "siq.get_data", "modulename": "siq.get_data", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "siq.get_data.DATA_PATH", "modulename": "siq.get_data", "qualname": "DATA_PATH", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;/Users/stnava/.siq/&#x27;"}, {"fullname": "siq.get_data.get_data", "modulename": "siq.get_data", "qualname": "get_data", "kind": "function", "doc": "<p>Get SIQ data filename</p>\n\n<p>The first time this is called, it will download data to ~/.siq.\nAfter, it will just read data from disk.  The ~/.siq may need to\nbe periodically deleted in order to ensure data is current.</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>name : string\n    name of data tag to retrieve\n    Options:\n        - 'all'</p>\n\n<p>force_download: boolean</p>\n\n<p>version: version of data to download (integer)</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>string\n    filepath of selected data</p>\n\n<h2 id=\"example\">Example</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">import</span> <span class=\"nn\">siq</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">siq</span><span class=\"o\">.</span><span class=\"n\">get_data</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">force_download</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">version</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"n\">target_extension</span><span class=\"o\">=</span><span class=\"s1\">&#39;.csv&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.dbpn", "modulename": "siq.get_data", "qualname": "dbpn", "kind": "function", "doc": "<p>Creates a Deep Back-Projection Network (DBPN) for single image super-resolution.</p>\n\n<p>This function constructs a Keras model based on the DBPN architecture, which\ncan be configured for either 2D or 3D inputs. The network uses iterative\nup- and down-projection blocks to refine the high-resolution image estimate. A\nkey modification from the original paper is the option to use standard\ninterpolation for upsampling instead of deconvolution layers.</p>\n\n<p>Reference:</p>\n\n<ul>\n<li>Haris, M., Shakhnarovich, G., &amp; Ukita, N. (2018). Deep Back-Projection\nNetworks For Super-Resolution. In CVPR.</li>\n</ul>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_image_size : tuple or list\n    The shape of the input image, including the channel.\n    e.g., <code>(None, None, 1)</code> for 2D or <code>(None, None, None, 1)</code> for 3D.</p>\n\n<p>number_of_outputs : int, optional\n    The number of channels in the output image. Default is 1.</p>\n\n<p>number_of_base_filters : int, optional\n    The number of filters in the up/down projection blocks. Default is 64.</p>\n\n<p>number_of_feature_filters : int, optional\n    The number of filters in the initial feature extraction layer. Default is 256.</p>\n\n<p>number_of_back_projection_stages : int, optional\n    The number of iterative back-projection stages (T in the paper). Default is 7.</p>\n\n<p>convolution_kernel_size : tuple or list, optional\n    The kernel size for the main projection convolutions. Should match the\n    dimensionality of the input. Default is (12, 12).</p>\n\n<p>strides : tuple or list, optional\n    The strides for the up/down sampling operations, defining the\n    super-resolution factor. Default is (8, 8).</p>\n\n<p>last_convolution : tuple or list, optional\n    The kernel size of the final reconstruction convolution. Default is (3, 3).</p>\n\n<p>number_of_loss_functions : int, optional\n    If greater than 1, the model will have multiple identical output branches.\n    Typically set to 1. Default is 1.</p>\n\n<p>interpolation : str, optional\n    The interpolation method to use for upsampling layers if not using\n    transposed convolutions. 'nearest' or 'bilinear'. Default is 'nearest'.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>keras.Model\n    A Keras model implementing the DBPN architecture for the specified\n    parameters.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_image_size</span>,</span><span class=\"param\">\t<span class=\"n\">number_of_outputs</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">number_of_base_filters</span><span class=\"o\">=</span><span class=\"mi\">64</span>,</span><span class=\"param\">\t<span class=\"n\">number_of_feature_filters</span><span class=\"o\">=</span><span class=\"mi\">256</span>,</span><span class=\"param\">\t<span class=\"n\">number_of_back_projection_stages</span><span class=\"o\">=</span><span class=\"mi\">7</span>,</span><span class=\"param\">\t<span class=\"n\">convolution_kernel_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"mi\">12</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">last_convolution</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">number_of_loss_functions</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">interpolation</span><span class=\"o\">=</span><span class=\"s1\">&#39;nearest&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.get_random_base_ind", "modulename": "siq.get_data", "qualname": "get_random_base_ind", "kind": "function", "doc": "<p>Generates a random top-left corner index for a patch.</p>\n\n<p>This utility function computes a valid starting index (e.g., [x, y, z])\nfor extracting a patch from a larger volume, ensuring the patch fits entirely\nwithin the volume's boundaries, accounting for an offset.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>full_dims : tuple or list\n    The dimensions of the full volume (e.g., img.shape).</p>\n\n<p>patchWidth : tuple or list\n    The dimensions of the patch to be extracted.</p>\n\n<p>off : int, optional\n    An offset from the edge of the volume to avoid sampling near borders.\n    Default is 8.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>list\n    A list of integers representing the starting coordinates for the patch.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">full_dims</span>, </span><span class=\"param\"><span class=\"n\">patchWidth</span>, </span><span class=\"param\"><span class=\"n\">off</span><span class=\"o\">=</span><span class=\"mi\">8</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.get_random_patch", "modulename": "siq.get_data", "qualname": "get_random_patch", "kind": "function", "doc": "<p>Extracts a random patch from an image with non-zero variance.</p>\n\n<p>This function repeatedly samples a random patch of a specified width from\nthe input image until it finds one where the standard deviation of pixel\nintensities is greater than zero. This is useful for avoiding blank or\nuniform patches during training data generation.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>img : ants.ANTsImage\n    The source image from which to extract a patch.</p>\n\n<p>patchWidth : tuple or list\n    The desired dimensions of the output patch.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>ants.ANTsImage\n    A randomly extracted patch from the input image.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">img</span>, </span><span class=\"param\"><span class=\"n\">patchWidth</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.get_random_patch_pair", "modulename": "siq.get_data", "qualname": "get_random_patch_pair", "kind": "function", "doc": "<p>Extracts a corresponding random patch from a pair of images.</p>\n\n<p>This function finds a single random location and extracts a patch of the\nsame size and position from two different input images. It ensures that\nboth extracted patches have non-zero variance. This is useful for creating\npaired training data (e.g., low-res and high-res images).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>img : ants.ANTsImage\n    The first source image.</p>\n\n<p>img2 : ants.ANTsImage\n    The second source image, spatially aligned with the first.</p>\n\n<p>patchWidth : tuple or list\n    The desired dimensions of the output patches.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tuple of ants.ANTsImage\n    A tuple containing two corresponding patches: (patch_from_img, patch_from_img2).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">img</span>, </span><span class=\"param\"><span class=\"n\">img2</span>, </span><span class=\"param\"><span class=\"n\">patchWidth</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.pseudo_3d_vgg_features", "modulename": "siq.get_data", "qualname": "pseudo_3d_vgg_features", "kind": "function", "doc": "<p>Creates a pseudo-3D VGG feature extractor from a pre-trained 2D VGG model.</p>\n\n<p>This function constructs a 3D VGG-style network and initializes its weights\nby \"stretching\" the weights from a pre-trained 2D VGG19 model (trained on\nImageNet) along a specified axis. This is a technique to transfer 2D\nperceptual knowledge to a 3D domain for tasks like perceptual loss.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>inshape : list of int, optional\n    The input shape of the 3D volume, e.g., <code>[128, 128, 128]</code>. Default is <code>[128,128,128]</code>.</p>\n\n<p>layer : int, optional\n    The block number of the VGG network from which to extract features. For\n    VGG19, this corresponds to block <code>layer</code> (e.g., layer=4 means 'block4_conv...').\n    Default is 4.</p>\n\n<p>angle : int, optional\n    The axis along which to project the 2D weights:\n    - 0: Axial plane (stretches along Z)\n    - 1: Coronal plane (stretches along Y)\n    - 2: Sagittal plane (stretches along X)\n    Default is 0.</p>\n\n<p>pretrained : bool, optional\n    If True, loads the stretched ImageNet weights. If False, the model is\n    randomly initialized. Default is True.</p>\n\n<p>verbose : bool, optional\n    If True, prints information about the layers being used. Default is False.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tf.keras.Model\n    A Keras model that takes a 3D volume as input and outputs the pseudo-3D\n    feature map from the specified layer and angle.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">inshape</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">layer</span><span class=\"o\">=</span><span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">angle</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">pretrained</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.pseudo_3d_vgg_features_unbiased", "modulename": "siq.get_data", "qualname": "pseudo_3d_vgg_features_unbiased", "kind": "function", "doc": "<p>Create a pseudo-3D VGG-style feature extractor by aggregating axial, coronal,\nand sagittal VGG feature representations.</p>\n\n<p>This model extracts features along each principal axis using pre-trained 2D\nVGG-style networks and concatenates them to form an unbiased pseudo-3D feature space.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>inshape : list of int, optional\n    The input shape of the 3D volume, default is [128, 128, 128].</p>\n\n<p>layer : int, optional\n    The VGG feature layer to extract. Higher values correspond to deeper\n    layers in the pseudo-3D VGG backbone.</p>\n\n<p>verbose : bool, optional\n    If True, prints debug messages during model construction.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tf.keras.Model\n    A TensorFlow Keras model that takes a 3D input volume and outputs the\n    concatenated pseudo-3D feature representation from the specified layer.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>This is useful for perceptual loss or feature comparison in super-resolution\nand image synthesis tasks. The same input is processed in three anatomical\nplanes (axial, coronal, sagittal), and features are concatenated.</p>\n\n<h2 id=\"see-also\">See Also</h2>\n\n<p>pseudo_3d_vgg_features : Generates VGG features from a single anatomical plane.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">inshape</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">layer</span><span class=\"o\">=</span><span class=\"mi\">4</span>, </span><span class=\"param\"><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.get_grader_feature_network", "modulename": "siq.get_data", "qualname": "get_grader_feature_network", "kind": "function", "doc": "<p>Load and extract a ResNet-based feature subnetwork for perceptual loss or quality grading.</p>\n\n<p>This function loads a pre-trained 3D ResNet model (\"grader\") used for\nperceptual feature extraction and returns a subnetwork that outputs activations\nfrom a specified internal layer.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>layer : int, optional\n    The index of the internal ResNet layer whose output should be used as\n    the feature representation. Default is layer 6.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tf.keras.Model\n    A Keras model that outputs features from the specified layer of the\n    pre-trained 3D ResNet grader model.</p>\n\n<h2 id=\"raises\">Raises</h2>\n\n<p>Exception\n    If the pre-trained weights file (<code>resnet_grader.h5</code>) is not found.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>The pre-trained weights should be located in: <code>~/.antspyt1w/resnet_grader.keras</code></p>\n\n<p>This model is typically used to compute perceptual loss by comparing\nintermediate activations between target and prediction volumes.</p>\n\n<h2 id=\"see-also\">See Also</h2>\n\n<p>antspynet.create_resnet_model_3d : Constructs the base ResNet model.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">layer</span><span class=\"o\">=</span><span class=\"mi\">6</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.default_dbpn", "modulename": "siq.get_data", "qualname": "default_dbpn", "kind": "function", "doc": "<p>Constructs a DBPN model based on input parameters, and can optionally\nuse external models for intensity or segmentation processing.</p>\n\n<p>Args:\n    strider (list): List of strides, length must match <code>dimensionality</code>.\n    dimensionality (int): Number of dimensions (2 or 3). Default is 3.\n    nfilt (int): Number of base filters. Default is 64.\n    nff (int): Number of feature filters. Default is 256.\n    convn (int): Convolution kernel size. Default is 6.\n    lastconv (int): Size of the last convolution. Default is 3.\n    nbp (int): Number of back projection stages. Default is 7.\n    nChannelsIn (int): Number of input channels. Default is 1.\n    nChannelsOut (int): Number of output channels. Default is 1.\n    option (str): Model size option ('tiny', 'small', 'medium', 'large'). Default is None.\n    intensity_model (tf.keras.Model): Optional external intensity model.\n    segmentation_model (tf.keras.Model): Optional external segmentation model.\n    sigmoid_second_channel (bool): If True, applies sigmoid to second channel in output.\n    clone_intensity_to_segmentation (bool): If True, clones intensity model weights to segmentation.\n    pro_seg (int): If greater than 0, adds a segmentation arm.\n    freeze (bool): If True, freezes the layers of the intensity/segmentation models.\n    verbose (bool): If True, prints detailed logs.</p>\n\n<p>Returns:\n    Model: A Keras model based on the specified configuration.</p>\n\n<p>Raises:\n    Exception: If <code>len(strider)</code> is not equal to <code>dimensionality</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">strider</span>,</span><span class=\"param\">\t<span class=\"n\">dimensionality</span><span class=\"o\">=</span><span class=\"mi\">3</span>,</span><span class=\"param\">\t<span class=\"n\">nfilt</span><span class=\"o\">=</span><span class=\"mi\">64</span>,</span><span class=\"param\">\t<span class=\"n\">nff</span><span class=\"o\">=</span><span class=\"mi\">256</span>,</span><span class=\"param\">\t<span class=\"n\">convn</span><span class=\"o\">=</span><span class=\"mi\">6</span>,</span><span class=\"param\">\t<span class=\"n\">lastconv</span><span class=\"o\">=</span><span class=\"mi\">3</span>,</span><span class=\"param\">\t<span class=\"n\">nbp</span><span class=\"o\">=</span><span class=\"mi\">7</span>,</span><span class=\"param\">\t<span class=\"n\">nChannelsIn</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">nChannelsOut</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">option</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">intensity_model</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">segmentation_model</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sigmoid_second_channel</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">clone_intensity_to_segmentation</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pro_seg</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">freeze</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.image_patch_training_data_from_filenames", "modulename": "siq.get_data", "qualname": "image_patch_training_data_from_filenames", "kind": "function", "doc": "<p>Generates a batch of paired high- and low-resolution image patches for training.</p>\n\n<p>This function creates training data by taking a list of high-resolution source\nimages, extracting random patches, and then downsampling them to create\nlow-resolution counterparts. This provides the (input, ground_truth) pairs\nneeded to train a super-resolution model.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filenames : list of str\n    A list of file paths to the high-resolution source images.</p>\n\n<p>target_patch_size : tuple or list of int\n    The dimensions of the high-resolution (ground truth) patch to extract,\n    e.g., <code>(128, 128, 128)</code>.</p>\n\n<p>target_patch_size_low : tuple or list of int\n    The dimensions of the low-resolution (input) patch to generate. The ratio\n    between <code>target_patch_size</code> and <code>target_patch_size_low</code> determines the\n    super-resolution factor.</p>\n\n<p>nPatches : int, optional\n    The number of patch pairs to generate in this batch. Default is 128.</p>\n\n<p>istest : bool, optional\n    If True, the function also generates a third output array containing patches\n    that have been naively upsampled using linear interpolation. This is useful\n    for calculating baseline evaluation metrics (e.g., PSNR) against which the\n    model's performance can be compared. Default is False.</p>\n\n<p>patch_scaler : bool, optional\n    If True, scales the intensity of each high-resolution patch to the [0, 1]\n    range before creating the downsampled version. This can help with\n    training stability. Default is True.</p>\n\n<p>to_tensorflow : bool, optional\n    If True, casts the output NumPy arrays to TensorFlow tensors. Default is False.</p>\n\n<p>verbose : bool, optional\n    If True, prints progress messages during patch generation. Default is False.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tuple\n    A tuple of NumPy arrays or TensorFlow tensors.\n    - If <code>istest</code> is False: <code>(patchesResam, patchesOrig)</code>\n        - <code>patchesResam</code>: The batch of low-resolution input patches (X_train).\n        - <code>patchesOrig</code>: The batch of high-resolution ground truth patches (y_train).\n    - If <code>istest</code> is True: <code>(patchesResam, patchesOrig, patchesUp)</code>\n        - <code>patchesUp</code>: The batch of baseline, linearly-upsampled patches.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filenames</span>,</span><span class=\"param\">\t<span class=\"n\">target_patch_size</span>,</span><span class=\"param\">\t<span class=\"n\">target_patch_size_low</span>,</span><span class=\"param\">\t<span class=\"n\">nPatches</span><span class=\"o\">=</span><span class=\"mi\">128</span>,</span><span class=\"param\">\t<span class=\"n\">istest</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">patch_scaler</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensorflow</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.seg_patch_training_data_from_filenames", "modulename": "siq.get_data", "qualname": "seg_patch_training_data_from_filenames", "kind": "function", "doc": "<p>Generates a batch of paired training data containing both images and segmentations.</p>\n\n<p>This function extends <code>image_patch_training_data_from_filenames</code> by adding a\nsecond channel to the data. For each extracted image patch, it also generates\na corresponding segmentation mask using Otsu's thresholding. This is useful for\ntraining multi-task models that perform super-resolution on both an image and\nits associated segmentation simultaneously.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filenames : list of str\n    A list of file paths to the high-resolution source images.</p>\n\n<p>target_patch_size : tuple or list of int\n    The dimensions of the high-resolution patch, e.g., <code>(128, 128, 128)</code>.</p>\n\n<p>target_patch_size_low : tuple or list of int\n    The dimensions of the low-resolution input patch.</p>\n\n<p>nPatches : int, optional\n    The number of patch pairs to generate. Default is 128.</p>\n\n<p>istest : bool, optional\n    If True, also generates a third output array containing baseline upsampled\n    intensity images (channel 0 only). Default is False.</p>\n\n<p>patch_scaler : bool, optional\n    If True, scales the intensity of each image patch to the [0, 1] range.\n    Default is True.</p>\n\n<p>to_tensorflow : bool, optional\n    If True, casts the output NumPy arrays to TensorFlow tensors. Default is False.</p>\n\n<p>verbose : bool, optional\n    If True, prints progress messages. Default is False.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tuple\n    A tuple of multi-channel NumPy arrays or TensorFlow tensors. The structure\n    is the same as <code>image_patch_training_data_from_filenames</code>, but each\n    array has a channel dimension of 2:\n    - Channel 0: The intensity image.\n    - Channel 1: The binary segmentation mask.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filenames</span>,</span><span class=\"param\">\t<span class=\"n\">target_patch_size</span>,</span><span class=\"param\">\t<span class=\"n\">target_patch_size_low</span>,</span><span class=\"param\">\t<span class=\"n\">nPatches</span><span class=\"o\">=</span><span class=\"mi\">128</span>,</span><span class=\"param\">\t<span class=\"n\">istest</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">patch_scaler</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensorflow</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.read", "modulename": "siq.get_data", "qualname": "read", "kind": "function", "doc": "<p>Reads an image or a NumPy array from a file.</p>\n\n<p>This function acts as a wrapper to intelligently load data. It checks the\nfile extension to decide whether to use <code>ants.image_read</code> for standard\nmedical image formats (e.g., .nii.gz, .mha) or <code>numpy.load</code> for <code>.npy</code> files.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filename : str\n    The full path to the file to be read.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>ants.ANTsImage or np.ndarray\n    The loaded data object, either as an ANTsImage or a NumPy array.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">filename</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.auto_weight_loss", "modulename": "siq.get_data", "qualname": "auto_weight_loss", "kind": "function", "doc": "<p>Automatically compute weighting coefficients for a combined loss function\nbased on intensity (MSE), perceptual similarity (feature), and total variation (TV).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>mdl : tf.keras.Model\n    A trained or untrained model to evaluate predictions on input <code>x</code>.</p>\n\n<p>feature_extractor : tf.keras.Model\n    A model that extracts intermediate features from the input. Commonly a VGG or ResNet\n    trained on a perceptual task.</p>\n\n<p>x : tf.Tensor\n    Input batch to the model.</p>\n\n<p>y : tf.Tensor\n    Ground truth target for <code>x</code>, typically a batch of 2D or 3D volumes.</p>\n\n<p>feature : float, optional\n    Weighting factor for the feature (perceptual) term in the loss. Default is 2.0.</p>\n\n<p>tv : float, optional\n    Weighting factor for the total variation term in the loss. Default is 0.1.</p>\n\n<p>verbose : bool, optional\n    If True, prints each component of the loss and its scaled value.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>list of float\n    A list of computed weights in the order:\n    <code>[msq_weight, feature_weight, tv_weight]</code></p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>The total loss (to be used during training) can then be constructed as:</p>\n\n<pre><code>`L = msq_weight * MSE + feature_weight * perceptual_loss + tv_weight * TV`\n</code></pre>\n\n<p>This function is typically used to balance loss terms before training.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">mdl</span>, </span><span class=\"param\"><span class=\"n\">feature_extractor</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">feature</span><span class=\"o\">=</span><span class=\"mf\">2.0</span>, </span><span class=\"param\"><span class=\"n\">tv</span><span class=\"o\">=</span><span class=\"mf\">0.1</span>, </span><span class=\"param\"><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.auto_weight_loss_seg", "modulename": "siq.get_data", "qualname": "auto_weight_loss_seg", "kind": "function", "doc": "<p>Automatically compute weighting coefficients for a combined loss function\nthat includes MSE, perceptual similarity, total variation, and segmentation Dice loss.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>mdl : tf.keras.Model\n    A segmentation + super-resolution model that outputs both image and label predictions.</p>\n\n<p>feature_extractor : tf.keras.Model\n    Feature extractor model used to compute perceptual similarity loss.</p>\n\n<p>x : tf.Tensor\n    Input tensor to the model.</p>\n\n<p>y : tf.Tensor\n    Target tensor with two channels: [intensity_image, segmentation_label].</p>\n\n<p>feature : float, optional\n    Relative weight of the perceptual feature loss term. Default is 2.0.</p>\n\n<p>tv : float, optional\n    Relative weight of the total variation (TV) term. Default is 0.1.</p>\n\n<p>dice : float, optional\n    Relative weight of the Dice loss term (for segmentation agreement). Default is 0.5.</p>\n\n<p>verbose : bool, optional\n    If True, prints the scaled values of each component loss.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>list of float\n    A list of loss term weights in the order:\n    <code>[msq_weight, feature_weight, tv_weight, dice_weight]</code></p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<ul>\n<li>The input and output tensors must be shaped such that the last axis is 2:\nchannel 0 is intensity, channel 1 is segmentation.</li>\n<li>This is useful for dual-task networks that predict both high-res images\nand associated segmentation masks.</li>\n</ul>\n\n<h2 id=\"see-also\">See Also</h2>\n\n<p>binary_dice_loss : Computes Dice loss between predicted and ground-truth masks.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">mdl</span>,</span><span class=\"param\">\t<span class=\"n\">feature_extractor</span>,</span><span class=\"param\">\t<span class=\"n\">x</span>,</span><span class=\"param\">\t<span class=\"n\">y</span>,</span><span class=\"param\">\t<span class=\"n\">feature</span><span class=\"o\">=</span><span class=\"mf\">2.0</span>,</span><span class=\"param\">\t<span class=\"n\">tv</span><span class=\"o\">=</span><span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">dice</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.numpy_generator", "modulename": "siq.get_data", "qualname": "numpy_generator", "kind": "function", "doc": "<p>A placeholder or stub for a data generator.</p>\n\n<p>This generator yields a tuple of <code>None</code> values once and then stops. It is\nlikely intended as a template or for debugging purposes where a generator\nobject is required but no actual data needs to be processed.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filenames : any\n    An argument that is not used by the function.</p>\n\n<h2 id=\"yields\">Yields</h2>\n\n<p>tuple\n    A single tuple <code>(None, None, None)</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">filenames</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.image_generator", "modulename": "siq.get_data", "qualname": "image_generator", "kind": "function", "doc": "<p>Creates an infinite generator of paired image patches for model training.</p>\n\n<p>This function continuously generates batches of low-resolution (input) and\nhigh-resolution (ground truth) image patches. It is designed to be fed\ndirectly into a Keras <code>model.fit()</code> call.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filenames : list of str\n    List of file paths to the high-resolution source images.\nnPatches : int\n    The number of patch pairs to generate and yield in each batch.\ntarget_patch_size : tuple or list of int\n    The dimensions of the high-resolution (ground truth) patches.\ntarget_patch_size_low : tuple or list of int\n    The dimensions of the low-resolution (input) patches.\npatch_scaler : bool, optional\n    If True, scales patch intensities to [0, 1]. Default is True.\nistest : bool, optional\n    If True, the generator will also yield a third item: a baseline\n    linearly upsampled version of the low-resolution patch for comparison.\n    Default is False.\nverbose : bool, optional\n    If True, passes verbosity to the underlying patch generation function.\n    Default is False.</p>\n\n<h2 id=\"yields\">Yields</h2>\n\n<p>tuple\n    A tuple of TensorFlow tensors ready for training or evaluation.\n    - If <code>istest</code> is False: <code>(low_res_batch, high_res_batch)</code>\n    - If <code>istest</code> is True: <code>(low_res_batch, high_res_batch, baseline_upsampled_batch)</code></p>\n\n<h2 id=\"see-also\">See Also</h2>\n\n<p>image_patch_training_data_from_filenames : The function that performs the\n                                           underlying patch extraction.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filenames</span>,</span><span class=\"param\">\t<span class=\"n\">nPatches</span>,</span><span class=\"param\">\t<span class=\"n\">target_patch_size</span>,</span><span class=\"param\">\t<span class=\"n\">target_patch_size_low</span>,</span><span class=\"param\">\t<span class=\"n\">patch_scaler</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">istest</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.seg_generator", "modulename": "siq.get_data", "qualname": "seg_generator", "kind": "function", "doc": "<p>Creates an infinite generator of paired image and segmentation patches.</p>\n\n<p>This function continuously generates batches of multi-channel patches, where\none channel is the intensity image and the other is a segmentation mask.\nIt is designed for training multi-task super-resolution models.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>filenames : list of str\n    List of file paths to the high-resolution source images.\nnPatches : int\n    The number of patch pairs to generate and yield in each batch.\ntarget_patch_size : tuple or list of int\n    The dimensions of the high-resolution patches.\ntarget_patch_size_low : tuple or list of int\n    The dimensions of the low-resolution patches.\npatch_scaler : bool, optional\n    If True, scales the intensity channel of patches to [0, 1]. Default is True.\nistest : bool, optional\n    If True, yields an additional baseline upsampled patch for comparison.\n    Default is False.\nverbose : bool, optional\n    If True, passes verbosity to the underlying patch generation function.\n    Default is False.</p>\n\n<h2 id=\"yields\">Yields</h2>\n\n<p>tuple\n    A tuple of multi-channel TensorFlow tensors. Each tensor has two channels:\n    Channel 0 contains the intensity image, and Channel 1 contains the\n    segmentation mask.</p>\n\n<h2 id=\"see-also\">See Also</h2>\n\n<p>seg_patch_training_data_from_filenames : The function that performs the\n                                         underlying patch extraction.\nimage_generator : A similar generator for intensity-only data.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">filenames</span>,</span><span class=\"param\">\t<span class=\"n\">nPatches</span>,</span><span class=\"param\">\t<span class=\"n\">target_patch_size</span>,</span><span class=\"param\">\t<span class=\"n\">target_patch_size_low</span>,</span><span class=\"param\">\t<span class=\"n\">patch_scaler</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">istest</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.train", "modulename": "siq.get_data", "qualname": "train", "kind": "function", "doc": "<p>Orchestrates the training process for a super-resolution model.</p>\n\n<p>This function handles the entire training loop, including setting up data\ngenerators, defining a composite loss function, automatically balancing loss\nweights, iteratively training the model, periodically evaluating performance,\nand saving the best-performing model weights.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>mdl : tf.keras.Model\n    The Keras model to be trained.\nfilenames_train : list of str\n    List of file paths for the training dataset.\nfilenames_test : list of str\n    List of file paths for the validation/testing dataset.\ntarget_patch_size : tuple or list\n    The dimensions of the high-resolution target patches.\ntarget_patch_size_low : tuple or list\n    The dimensions of the low-resolution input patches.\noutput_prefix : str\n    A prefix for all output files (e.g., model weights, training logs).\nn_test : int, optional\n    The number of validation patches to use for evaluation. Default is 8.\nlearning_rate : float, optional\n    The learning rate for the Adam optimizer. Default is 5e-5.\nfeature_layer : int, optional\n    The layer index from the feature extractor to use for perceptual loss.\n    Default is 6.\nfeature : float, optional\n    The relative weight of the perceptual (feature) loss term. Default is 2.0.\ntv : float, optional\n    The relative weight of the Total Variation (TV) regularization term.\n    Default is 0.1.\nmax_iterations : int, optional\n    The total number of training iterations to run. Default is 1000.\nbatch_size : int, optional\n    The batch size for training. Note: this implementation is optimized for\n    batch_size=1 and may need adjustment for larger batches. Default is 1.\nsave_all_best : bool, optional\n    If True, saves a new model file every time validation loss improves.\n    If False, overwrites the single best model file. Default is False.\nfeature_type : str, optional\n    The type of feature extractor for perceptual loss. Options: 'grader',\n    'vgg', 'vggrandom'. Default is 'grader'.\ncheck_eval_data_iteration : int, optional\n    The frequency (in iterations) at which to run validation and save logs.\n    Default is 20.\nverbose : bool, optional\n    If True, prints detailed progress information. Default is False.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>pd.DataFrame\n    A DataFrame containing the training history, with columns for training\n    loss, validation loss, PSNR, and baseline PSNR over iterations.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">mdl</span>,</span><span class=\"param\">\t<span class=\"n\">filenames_train</span>,</span><span class=\"param\">\t<span class=\"n\">filenames_test</span>,</span><span class=\"param\">\t<span class=\"n\">target_patch_size</span>,</span><span class=\"param\">\t<span class=\"n\">target_patch_size_low</span>,</span><span class=\"param\">\t<span class=\"n\">output_prefix</span>,</span><span class=\"param\">\t<span class=\"n\">n_test</span><span class=\"o\">=</span><span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">5e-05</span>,</span><span class=\"param\">\t<span class=\"n\">feature_layer</span><span class=\"o\">=</span><span class=\"mi\">6</span>,</span><span class=\"param\">\t<span class=\"n\">feature</span><span class=\"o\">=</span><span class=\"mi\">2</span>,</span><span class=\"param\">\t<span class=\"n\">tv</span><span class=\"o\">=</span><span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">max_iterations</span><span class=\"o\">=</span><span class=\"mi\">1000</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">save_all_best</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">feature_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;grader&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">check_eval_data_iteration</span><span class=\"o\">=</span><span class=\"mi\">20</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.binary_dice_loss", "modulename": "siq.get_data", "qualname": "binary_dice_loss", "kind": "function", "doc": "<p>Computes the Dice loss for binary segmentation tasks.</p>\n\n<p>The Dice coefficient is a common metric for comparing the overlap of two samples.\nThis loss function computes <code>1 - DiceCoefficient</code>, making it suitable for\nminimization during training. A smoothing factor is added to avoid division\nby zero when both the prediction and the ground truth are empty.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>y_true : tf.Tensor\n    The ground truth binary segmentation mask. Values should be 0 or 1.\ny_pred : tf.Tensor\n    The predicted binary segmentation mask, typically with values in [0, 1]\n    from a sigmoid activation.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>tf.Tensor\n    A scalar tensor representing the Dice loss. The value ranges from -1 (perfect\n    match) to 0 (no overlap), though it's typically used as <code>1 - dice_coeff</code>\n    or just <code>-dice_coeff</code> (as here).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">y_true</span>, </span><span class=\"param\"><span class=\"n\">y_pred</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.train_seg", "modulename": "siq.get_data", "qualname": "train_seg", "kind": "function", "doc": "<p>Orchestrates training for a multi-task image and segmentation SR model.</p>\n\n<p>This function extends the <code>train</code> function to handle models that predict\nboth a super-resolved image and a super-resolved segmentation mask. It uses\na four-component composite loss: MSE (for image), a perceptual loss (for\nimage), Total Variation (for image), and Dice loss (for segmentation).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>mdl : tf.keras.Model\n    The 2-channel Keras model to be trained.\nfilenames_train : list of str\n    List of file paths for the training dataset.\nfilenames_test : list of str\n    List of file paths for the validation/testing dataset.\ntarget_patch_size : tuple or list\n    The dimensions of the high-resolution target patches.\ntarget_patch_size_low : tuple or list\n    The dimensions of the low-resolution input patches.\noutput_prefix : str\n    A prefix for all output files.\nn_test : int, optional\n    Number of validation patches for evaluation. Default is 8.\nlearning_rate : float, optional\n    Learning rate for the Adam optimizer. Default is 5e-5.\nfeature_layer : int, optional\n    Layer from the feature extractor for perceptual loss. Default is 6.\nfeature : float, optional\n    Relative weight of the perceptual loss term. Default is 2.0.\ntv : float, optional\n    Relative weight of the Total Variation regularization term. Default is 0.1.\ndice : float, optional\n    Relative weight of the Dice loss term for the segmentation mask.\n    Default is 0.5.\nmax_iterations : int, optional\n    Total number of training iterations. Default is 1000.\nbatch_size : int, optional\n    The batch size for training. Default is 1.\nsave_all_best : bool, optional\n    If True, saves all models that improve validation loss. Default is False.\nfeature_type : str, optional\n    Type of feature extractor for perceptual loss. Default is 'grader'.\ncheck_eval_data_iteration : int, optional\n    Frequency (in iterations) for running validation. Default is 20.\nverbose : bool, optional\n    If True, prints detailed progress information. Default is False.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>pd.DataFrame\n    A DataFrame containing the training history, including columns for losses\n    and evaluation metrics like PSNR and Dice score.</p>\n\n<h2 id=\"see-also\">See Also</h2>\n\n<p>train : The training function for single-task (intensity-only) models.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">mdl</span>,</span><span class=\"param\">\t<span class=\"n\">filenames_train</span>,</span><span class=\"param\">\t<span class=\"n\">filenames_test</span>,</span><span class=\"param\">\t<span class=\"n\">target_patch_size</span>,</span><span class=\"param\">\t<span class=\"n\">target_patch_size_low</span>,</span><span class=\"param\">\t<span class=\"n\">output_prefix</span>,</span><span class=\"param\">\t<span class=\"n\">n_test</span><span class=\"o\">=</span><span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">5e-05</span>,</span><span class=\"param\">\t<span class=\"n\">feature_layer</span><span class=\"o\">=</span><span class=\"mi\">6</span>,</span><span class=\"param\">\t<span class=\"n\">feature</span><span class=\"o\">=</span><span class=\"mi\">2</span>,</span><span class=\"param\">\t<span class=\"n\">tv</span><span class=\"o\">=</span><span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">dice</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">max_iterations</span><span class=\"o\">=</span><span class=\"mi\">1000</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">save_all_best</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">feature_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;grader&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">check_eval_data_iteration</span><span class=\"o\">=</span><span class=\"mi\">20</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.read_srmodel", "modulename": "siq.get_data", "qualname": "read_srmodel", "kind": "function", "doc": "<p>Load a super-resolution model (h5, .keras, or SavedModel format),\nand determine its upsampling factor.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>srfilename : str\n    Path to the model file (.h5, .keras, or a SavedModel folder).\ncustom_objects : dict, optional\n    Dictionary of custom objects used in the model (e.g. {'TFOpLambda': tf.keras.layers.Lambda(...)})</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>model : tf.keras.Model\n    The loaded model.\nupsampling_factor : list of int\n    List describing the upsampling factor:\n    - For 3D input: [x_up, y_up, z_up, channels]\n    - For 2D input: [x_up, y_up, channels]</p>\n\n<h2 id=\"example\">Example</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">mdl</span><span class=\"p\">,</span> <span class=\"n\">up</span> <span class=\"o\">=</span> <span class=\"n\">read_srmodel</span><span class=\"p\">(</span><span class=\"s2\">&quot;mymodel.keras&quot;</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">mdl</span><span class=\"p\">,</span> <span class=\"n\">up</span> <span class=\"o\">=</span> <span class=\"n\">read_srmodel</span><span class=\"p\">(</span><span class=\"s2\">&quot;my_weights.h5&quot;</span><span class=\"p\">,</span> <span class=\"n\">custom_objects</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">&quot;TFOpLambda&quot;</span><span class=\"p\">:</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Lambda</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">identity</span><span class=\"p\">)})</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">srfilename</span>, </span><span class=\"param\"><span class=\"n\">custom_objects</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.simulate_image", "modulename": "siq.get_data", "qualname": "simulate_image", "kind": "function", "doc": "<p>generate an image of given shape and number of levels</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>shaper : [x,y,z] or [x,y]</p>\n\n<p>n_levels : int</p>\n\n<p>multiply : boolean</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>ants.image</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">shaper</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">n_levels</span><span class=\"o\">=</span><span class=\"mi\">10</span>, </span><span class=\"param\"><span class=\"n\">multiply</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.optimize_upsampling_shape", "modulename": "siq.get_data", "qualname": "optimize_upsampling_shape", "kind": "function", "doc": "<p>Compute the optimal upsampling shape string (e.g., '2x2x2') based on image voxel spacing\nand imaging modality. This output is used to select an appropriate pretrained \nsuper-resolution model filename.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>spacing : sequence of float\n    Voxel spacing (physical size per voxel in mm) from the input image.\n    Typically obtained from <code>ants.get_spacing(image)</code>.</p>\n\n<p>modality : str, optional\n    Imaging modality. Affects resolution thresholds:\n    - 'T1' : anatomical MRI (default minimum spacing: 0.35 mm)\n    - 'DTI' : diffusion MRI (default minimum spacing: 1.0 mm)\n    - 'NM' : nuclear medicine (e.g., PET/SPECT, minimum spacing: 0.25 mm)</p>\n\n<p>roundit : bool, optional\n    If True, uses rounded integer ratios for the upsampling shape.\n    Otherwise, uses floor division with constraints.</p>\n\n<p>verbose : bool, optional\n    If True, prints detailed internal values and logic.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>str\n    Optimal upsampling shape string in the form 'AxBxC',\n    e.g., '2x2x2', '4x4x2'.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<ul>\n<li>The function prevents upsampling ratios that would result in '1x1x1'\nby defaulting to '2x2x2'.</li>\n<li>It also avoids uncommon ratios like '5' by rounding to the nearest valid option.</li>\n<li><p>The returned string is commonly used to populate a model filename template:</p>\n\n<p>Example:</p>\n\n<blockquote>\n  <blockquote>\n    <blockquote>\n      <p>bestup = optimize_upsampling_shape(ants.get_spacing(t1_img), modality='T1')\n      model = re.sub('bestup', bestup, 'siq_smallshort_train_bestup_1chan.keras')</p>\n    </blockquote>\n  </blockquote>\n</blockquote></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">spacing</span>, </span><span class=\"param\"><span class=\"n\">modality</span><span class=\"o\">=</span><span class=\"s1\">&#39;T1&#39;</span>, </span><span class=\"param\"><span class=\"n\">roundit</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.compare_models", "modulename": "siq.get_data", "qualname": "compare_models", "kind": "function", "doc": "<p>Evaluates and compares the performance of multiple super-resolution models on a given image.</p>\n\n<p>This function provides a standardized way to benchmark SR models. For each model,\nit performs the following steps:</p>\n\n<ol>\n<li>Loads the model and determines its upsampling factor.</li>\n<li>Downsamples the high-resolution input image (<code>img</code>) to create a low-resolution\ninput, simulating a real-world scenario.</li>\n<li>Adds Gaussian noise to the low-resolution input to test for robustness.</li>\n<li>Runs inference using the model to generate a super-resolved output.</li>\n<li>Generates a baseline output by upsampling the low-res input with linear interpolation.</li>\n<li>Calculates PSNR and SSIM metrics comparing both the model's output and the\nbaseline against the original high-resolution image.</li>\n<li>If a dual-channel (image + segmentation) model is detected, it also calculates\nDice scores for segmentation performance.</li>\n<li>Aggregates all results into a pandas DataFrame for easy comparison.</li>\n</ol>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>model_filenames : list of str\n    A list of file paths to the Keras models (.h5, .keras) to be compared.\nimg : ants.ANTsImage\n    The high-resolution ground truth image. This image will be downsampled to\n    create the input for the models.\nn_classes : int, optional\n    The number of classes for Otsu's thresholding when auto-generating a\n    segmentation for evaluating dual-channel models. Default is 3.\npoly_order : str or int, optional\n    Method for intensity matching between the SR output and the reference.\n    Options: 'hist' for histogram matching (default), an integer for\n    polynomial regression, or None to disable.\nidentifier : str, optional\n    A custom identifier for the output DataFrame. If None, it is inferred\n    from the model filename. Default is None.\nnoise_sd : float, optional\n    Standard deviation of the additive Gaussian noise applied to the\n    downsampled image before inference. Default is 0.1.\nverbose : bool, optional\n    If True, prints detailed progress and intermediate values. Default is False.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>pd.DataFrame\n    A DataFrame where each row corresponds to a model. Columns contain evaluation\n    metrics (PSNR.SR, SSIM.SR, DICE.SR), baseline metrics (PSNR.LIN, SSIM.LIN,\n    DICE.NN), and metadata.</p>\n\n<h2 id=\"notes\">Notes</h2>\n\n<p>When evaluating a 2-channel (segmentation) model, the primary metric for the\nsegmentation task is the Dice score (<code>DICE.SR</code>). The intensity metrics (PSNR, SSIM)\nare still computed on the first channel.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_filenames</span>,</span><span class=\"param\">\t<span class=\"n\">img</span>,</span><span class=\"param\">\t<span class=\"n\">n_classes</span><span class=\"o\">=</span><span class=\"mi\">3</span>,</span><span class=\"param\">\t<span class=\"n\">poly_order</span><span class=\"o\">=</span><span class=\"s1\">&#39;hist&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">identifier</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">noise_sd</span><span class=\"o\">=</span><span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.region_wise_super_resolution", "modulename": "siq.get_data", "qualname": "region_wise_super_resolution", "kind": "function", "doc": "<p>Apply super-resolution model to each labeled region in the mask independently.</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>image : ANTsImage\n    Input image.</p>\n\n<p>mask : ANTsImage\n    Integer-labeled segmentation mask with non-zero regions to upsample.</p>\n\n<p>super_res_model : tf.keras.Model\n    Trained super-resolution model.</p>\n\n<p>dilation_amount : int\n    Number of morphological dilations applied to each label region before cropping.</p>\n\n<p>verbose : bool\n    If True, print detailed status.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>ANTsImage : Full-size super-resolved image with per-label inference and stitching.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">image</span>, </span><span class=\"param\"><span class=\"n\">mask</span>, </span><span class=\"param\"><span class=\"n\">super_res_model</span>, </span><span class=\"param\"><span class=\"n\">dilation_amount</span><span class=\"o\">=</span><span class=\"mi\">4</span>, </span><span class=\"param\"><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.region_wise_super_resolution_blended", "modulename": "siq.get_data", "qualname": "region_wise_super_resolution_blended", "kind": "function", "doc": "<p>Apply super-resolution model to labeled regions with smooth blending to minimize stitching artifacts.</p>\n\n<p>This version uses a weighted-averaging scheme based on distance transforms\nto create seamless transitions between super-resolved regions and the background.</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>image : ANTsImage\n    Input low-resolution image.</p>\n\n<p>mask : ANTsImage\n    Integer-labeled segmentation mask.</p>\n\n<p>super_res_model : tf.keras.Model\n    Trained super-resolution model.</p>\n\n<p>dilation_amount : int\n    Number of morphological dilations applied to each label region before cropping.\n    This provides context to the SR model.</p>\n\n<p>verbose : bool\n    If True, print detailed status.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>ANTsImage : Full-size, super-resolved image with seamless blending.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">image</span>, </span><span class=\"param\"><span class=\"n\">mask</span>, </span><span class=\"param\"><span class=\"n\">super_res_model</span>, </span><span class=\"param\"><span class=\"n\">dilation_amount</span><span class=\"o\">=</span><span class=\"mi\">4</span>, </span><span class=\"param\"><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "siq.get_data.inference", "modulename": "siq.get_data", "qualname": "inference", "kind": "function", "doc": "<p>Perform super-resolution inference on an input image, optionally guided by segmentation.</p>\n\n<p>This function uses a trained deep learning model to enhance the resolution of a medical image.\nIt optionally applies label-wise inference if a segmentation mask is provided.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>image : ants.ANTsImage\n    Input image to be super-resolved.</p>\n\n<p>mdl : keras.Model\n    Trained super-resolution model, typically from ANTsPyNet.</p>\n\n<p>truncation : tuple or list of float, optional\n    Percentile values (e.g., [0.01, 0.99]) for intensity truncation before model input.\n    If None, no truncation is applied.</p>\n\n<p>segmentation : ants.ANTsImage, optional\n    A labeled segmentation mask. If provided, super-resolution is performed per label\n    using <code>region_wise_super_resolution</code> or <code>super_resolution_segmentation_per_label</code>.</p>\n\n<p>target_range : list of float\n    Intensity range used for scaling the input before applying the model.\n    Default is [1, 0] (internal default for <code>apply_super_resolution_model_to_image</code>).</p>\n\n<p>poly_order : int, str or None\n    Determines how to match intensity between the super-resolved image and the original.\n    Options:\n      - 'hist' : use histogram matching\n      - int &gt;= 1 : perform polynomial regression of this order\n      - None : no intensity adjustment</p>\n\n<p>dilation_amount : int\n    Number of dilation steps applied to each segmentation label during\n    region-based super-resolution (if segmentation is provided).</p>\n\n<p>verbose : bool\n    If True, print progress and status messages.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>ANTsImage or dict\n    - If <code>segmentation</code> is None, returns a single ANTsImage (super-resolved image).\n    - If <code>segmentation</code> is provided, returns a dictionary with:\n        - 'super_resolution': ANTsImage\n        - other entries may include label-wise results or metadata.</p>\n\n<h2 id=\"examples\">Examples</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">import</span> <span class=\"nn\">ants</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">import</span> <span class=\"nn\">antspynet</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">siq</span> <span class=\"kn\">import</span> <span class=\"n\">inference</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">ants</span><span class=\"o\">.</span><span class=\"n\">image_read</span><span class=\"p\">(</span><span class=\"s2\">&quot;lowres.nii.gz&quot;</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">antspynet</span><span class=\"o\">.</span><span class=\"n\">get_pretrained_network</span><span class=\"p\">(</span><span class=\"s2\">&quot;dbpn&quot;</span><span class=\"p\">,</span> <span class=\"n\">target_suffix</span><span class=\"o\">=</span><span class=\"s2\">&quot;T1&quot;</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">srimg</span> <span class=\"o\">=</span> <span class=\"n\">inference</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">truncation</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"mf\">0.99</span><span class=\"p\">],</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">seg</span> <span class=\"o\">=</span> <span class=\"n\">ants</span><span class=\"o\">.</span><span class=\"n\">image_read</span><span class=\"p\">(</span><span class=\"s2\">&quot;mask.nii.gz&quot;</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">sr_result</span> <span class=\"o\">=</span> <span class=\"n\">inference</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">segmentation</span><span class=\"o\">=</span><span class=\"n\">seg</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">srimg</span> <span class=\"o\">=</span> <span class=\"n\">sr_result</span><span class=\"p\">[</span><span class=\"s1\">&#39;super_resolution&#39;</span><span class=\"p\">]</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image</span>,</span><span class=\"param\">\t<span class=\"n\">mdl</span>,</span><span class=\"param\">\t<span class=\"n\">truncation</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">segmentation</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">target_range</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">poly_order</span><span class=\"o\">=</span><span class=\"s1\">&#39;hist&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">dilation_amount</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();